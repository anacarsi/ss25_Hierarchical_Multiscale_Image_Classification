{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f598cbee",
   "metadata": {},
   "source": [
    "### **Train ResNetCLassifier and extract features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c086ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "from lxml import etree\n",
    "from torchvision import transforms\n",
    "\n",
    "os.add_dll_directory(\n",
    "    r\"C:\\Program Files\\OpenSlide\\openslide-bin-4.0.0.8-windows-x64\\bin\"\n",
    ")\n",
    "import openslide\n",
    "from models.resnet import ResNet18Classifier, ResNet18FeatureExtractor\n",
    "from datasets.patch_dataset import PatchDataset\n",
    "from PIL import Image\n",
    "\n",
    "def parse_xml_mask(xml_path, level_dims, downsample):\n",
    "    \"\"\"\n",
    "    Convert XML annotation to binary mask.\n",
    "    Parameters:\n",
    "    - xml_path: str, path to the XML file containing annotations.\n",
    "    - level_dims: tuple, dimensions of the WSI at the specified level (width, height).\n",
    "    - downsample: float, downsample factor for the WSI level.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = etree.parse(xml_path)\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        print(f\"Error parsing XML file {xml_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    mask = Image.new(\"L\", level_dims, 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    for coordinates_node in tree.xpath(\"//Annotation/Coordinates | //Annotations/Annotation/Coordinates\"):\n",
    "        coords = []\n",
    "        for coord_node in coordinates_node.findall(\"Coordinate\"):\n",
    "            try:\n",
    "                x = float(coord_node.get(\"X\"))\n",
    "                y = float(coord_node.get(\"Y\"))\n",
    "                # Scale coordinates to the target level\n",
    "                scaled_x = int(x / downsample)\n",
    "                scaled_y = int(y / downsample)\n",
    "                coords.append((scaled_x, scaled_y))\n",
    "            except (ValueError, TypeError) as e:\n",
    "                print(f\"Warning: Could not parse coordinate (X,Y) from XML for {xml_path}: {e}\")\n",
    "                continue\n",
    "        if coords:\n",
    "            # Draw with 255 for white on a black background\n",
    "            draw.polygon(coords, outline=255, fill=255)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def train_resnet_classifier(level=3):\n",
    "    \"\"\" \n",
    "    Train a ResNet18 classifier on the extracted patches.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Training ResNet18 classifier on extracted patches...\")\n",
    "    patch_dir = os.path.join(os.getcwd(), \"..\", \"data\", \"camelyon16\", \"patches\", f\"level_{level}\")\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    dataset = PatchDataset(patch_dir, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ResNet18Classifier().to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct = 0, 0\n",
    "        for imgs, labels, _ in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(dataset)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    torch.save(model.state_dict(), \"models/resnet18_patch_classifier.pth\")\n",
    "    print(\"[INFO] ResNet18 classifier training complete and saved.\")\n",
    "\n",
    "\n",
    "def extract_patches(patch_size=224, level=3, stride=None, pad=True):\n",
    "    \"\"\"\n",
    "    Extract patches from WSIs at a specified level, apply mask overlays, and save tumor vs normal labels.\n",
    "    Only extracts patches if they have not already been extracted for a given image.\n",
    "    Parameters:\n",
    "    - patch_size: int, size of the patches to extract.\n",
    "    - level: int, level of the WSI to extract patches from.\n",
    "    - stride: int, stride for patch extraction.\n",
    "    - pad: bool, if True, pad the image to cover all regions.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Extracting patches at level {level}...\")\n",
    "    stride = stride or patch_size\n",
    "\n",
    "    # Set patch size according to level\n",
    "    patch_sizes = {0: 1792, 1: 896, 2: 448, 3: 224}\n",
    "    patch_size = patch_sizes.get(level, 224)\n",
    "\n",
    "    wsi_dir = os.path.join(os.getcwd(), \"..\", \"data\", \"camelyon16\", \"train\", \"img\")\n",
    "    annot_dir_train = os.path.join(\n",
    "        os.getcwd(), \"..\", \"data\", \"camelyon16\", \"train\", \"mask\", \"annotations\"\n",
    "    )\n",
    "    annot_dir_test = os.path.join(\n",
    "        os.getcwd(), \"..\", \"data\", \"camelyon16\", \"test\", \"mask\", \"annotations\"\n",
    "    )\n",
    "    level_dir = os.path.join(\n",
    "        os.getcwd(), \"..\", \"data\", \"camelyon16\", \"patches\", f\"level_{level}\"\n",
    "    )\n",
    "    os.makedirs(level_dir, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(wsi_dir):\n",
    "        if not file.endswith(\".tif\"):\n",
    "            continue\n",
    "        prefix = file.replace(\".tif\", \"\")\n",
    "\n",
    "        # Check if patches for this image already exist\n",
    "        patch_save_dir = os.path.join(level_dir, prefix)\n",
    "        if os.path.exists(patch_save_dir) and len(os.listdir(patch_save_dir)) > 0:\n",
    "            print(f\"[INFO] Patches for {file} already extracted, skipping.\")\n",
    "            continue\n",
    "        os.makedirs(patch_save_dir, exist_ok=True)\n",
    "\n",
    "        wsi_path = os.path.join(wsi_dir, file)\n",
    "        xml_name = file.replace(\".tif\", \".xml\")\n",
    "        if file.startswith(\"test_\"):\n",
    "            xml_path = os.path.join(annot_dir_test, xml_name)\n",
    "        elif file.startswith(\"normal_\") or file.startswith(\"tumor_\"):\n",
    "            xml_path = os.path.join(annot_dir_train, xml_name)\n",
    "        print(f\"[DEBUG] Processing file: {wsi_path} with XML: {xml_path}\")\n",
    "        try:\n",
    "            slide = openslide.OpenSlide(wsi_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not open {wsi_path}: {e}\")\n",
    "            continue\n",
    "        downsample = slide.level_downsamples[level]\n",
    "        width, height = slide.level_dimensions[level]\n",
    "\n",
    "        # Calculate padded size if needed\n",
    "        if pad:\n",
    "            pad_w = (patch_size - width % patch_size) % patch_size\n",
    "            pad_h = (patch_size - height % patch_size) % patch_size\n",
    "            padded_width = width + pad_w\n",
    "            padded_height = height + pad_h\n",
    "        else:\n",
    "            padded_width = width\n",
    "            padded_height = height\n",
    "\n",
    "        # Load and render XML mask\n",
    "        mask = None\n",
    "        if os.path.exists(xml_path):\n",
    "            try:\n",
    "                mask = parse_xml_mask(xml_path, (width, height), downsample)\n",
    "                if pad and (pad_w > 0 or pad_h > 0):\n",
    "                    mask = ImageOps.expand(mask, (0, 0, pad_w, pad_h), fill=0)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Failed to parse XML for {file}: {e}\")\n",
    "        else:\n",
    "            print(f\"[INFO] No annotation found for {file}, treating as normal.\")\n",
    "\n",
    "        print(f\"[INFO] Processing {file} at level {level} (size: {width}x{height}, padded: {padded_width}x{padded_height})\")\n",
    "\n",
    "        patch_count = 0\n",
    "        for x in range(0, padded_width, stride):\n",
    "            for y in range(0, padded_height, stride):\n",
    "                # Only process if the top-left corner is inside the original image\n",
    "                if x >= width or y >= height:\n",
    "                    continue\n",
    "\n",
    "                patch_w = min(patch_size, width - x)\n",
    "                patch_h = min(patch_size, height - y)\n",
    "                if patch_w <= 0 or patch_h <= 0:\n",
    "                    continue\n",
    "\n",
    "                region = slide.read_region(\n",
    "                    (int(x * downsample), int(y * downsample)),\n",
    "                    level,\n",
    "                    (patch_w, patch_h),\n",
    "                ).convert(\"RGB\")\n",
    "\n",
    "                # If patch is smaller than patch_size (at border), pad it to patch_size\n",
    "                if patch_w < patch_size or patch_h < patch_size:\n",
    "                    padded_region = Image.new(\"RGB\", (patch_size, patch_size), (255, 255, 255))\n",
    "                    padded_region.paste(region, (0, 0))\n",
    "                    region = padded_region\n",
    "\n",
    "                label = \"normal\"\n",
    "                # Check if the patch overlaps with any positimve (tumor) region in the generated binary mask\n",
    "                if mask:\n",
    "                    mask_patch = mask.crop((x, y, x + patch_size, y + patch_size))\n",
    "                    if np.any(np.array(mask_patch) > 0):\n",
    "                        label = \"tumor\"\n",
    "\n",
    "                patch_array = np.array(region)\n",
    "                if np.mean(patch_array) > 240:  # too white (empty tissue)\n",
    "                    continue\n",
    "\n",
    "                patch_save_dir = os.path.join(level_dir, prefix)\n",
    "                os.makedirs(patch_save_dir, exist_ok=True)\n",
    "                patch_name = f\"{prefix}_x{x}_y{y}_{label}.png\"\n",
    "                region.save(os.path.join(patch_save_dir, patch_name))\n",
    "                patch_count += 1\n",
    "                if patch_count % 100 == 0:\n",
    "                    print(f\"Extracted patches {patch_count} for {file}\")\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Patch extraction complete for {file} at level {level}. Total patches: {patch_count}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def extract_features(level=3, model_path=\"resnet18_patch_classifier.pth\"):\n",
    "    \"\"\"\n",
    "    Extract features from the patches using a ResNet18 model.\n",
    "    Parameters:\n",
    "    - level: int, WSI level to extract patches from (0, 1, 2, 3).\n",
    "    - model_path: str, path to the pre-trained ResNet18 model.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    model_path = os.path.join(os.getcwd(), \"models\", model_path)\n",
    "    patch_dir = os.path.join(\n",
    "        os.getcwd(), \"..\", \"data\", \"camelyon16\", \"patches\", f\"level_{level}\"\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(patch_dir) or not os.listdir(patch_dir):\n",
    "        print(f\"[ERROR] Patch directory '{patch_dir}' does not exist or is empty. Please run patch extraction first.\")\n",
    "        return\n",
    "\n",
    "    dataset = PatchDataset(patch_dir, transform=transform)\n",
    "    # Use higher batch size and num_workers for feature extraction as it's typically I/O bound\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False, num_workers=os.cpu_count() or 1) \n",
    "    \n",
    "    print(\n",
    "        f\"[INFO] Extracting features from patches at level {level} with patch directory: {patch_dir}, which exists: {os.path.exists(patch_dir)}\"\n",
    "    )\n",
    "    print(\n",
    "        \"[INFO] Listing first 5 subdirectories in patch_dir:\",\n",
    "        os.listdir(patch_dir)[:5] if os.path.exists(patch_dir) else \"Not found\",\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ResNet18FeatureExtractor().to(device)\n",
    "    full_classifier_model = ResNet18Classifier().to(device)\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"[INFO] Loading trained classifier weights from {model_path}\")\n",
    "        full_classifier_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    else:\n",
    "        print(f\"[WARNING] Trained classifier model not found at {model_path}. \"\n",
    "              \"Extracting features with ImageNet pre-trained weights only. \"\n",
    "              \"Consider running `train_resnet_classifier()` first.\")\n",
    "\n",
    "    model = ResNet18FeatureExtractor().to(device)\n",
    "    # Load the state_dict and filter out the 'fc' layer weights\n",
    "    pretrained_dict = full_classifier_model.state_dict()\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and not k.startswith('model.fc')}\n",
    "\n",
    "    # copy params from pretrained_dict to model_dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    model.eval() \n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    paths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (imgs, lbls, img_paths) in enumerate(tqdm(loader, desc=\"Extracting Features\")):\n",
    "            # print(\"Batch size:\", imgs.shape)\n",
    "            feats = model(imgs.to(device))\n",
    "            features.append(feats.cpu())\n",
    "            labels.extend(lbls.tolist()) # Convert tensor to list for extend\n",
    "            paths.extend(img_paths)\n",
    "    \n",
    "    if not features:\n",
    "        print(\n",
    "            \"[ERROR] No features were extracted. Check your patch directory and dataset. \"\n",
    "            \"It might be that PatchDataset found no images, or data loader was empty.\"\n",
    "        )\n",
    "        return\n",
    "        \n",
    "    features = torch.cat(features, dim=0)  # (num_patches, 512)\n",
    "\n",
    "    # Save features, labels, and paths\n",
    "    features_save_path = f\"patch_features_{level}.npy\"\n",
    "    labels_save_path = f\"patch_labels_{level}.npy\"\n",
    "    paths_save_path = f\"patch_paths_{level}.txt\"\n",
    "    \n",
    "    np.save(features_save_path, features.numpy())\n",
    "    np.save(labels_save_path, np.array(labels))\n",
    "    with open(paths_save_path, \"w\") as f:\n",
    "        for p in paths:\n",
    "            f.write(f\"{p}\\n\")\n",
    "    print(f\"[INFO] Features saved to {features_save_path}, labels to {labels_save_path}, paths to {paths_save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2529aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting features from patches at level 3 with patch directory: c:\\Users\\anaca\\Documents\\sexto.curso\\tfg info\\fresh-clone\\ss25_Hierarchical_Multiscale_Image_Classification\\src\\..\\data\\camelyon16\\patches\\level_3, which exists: True\n",
      "[INFO] Listing first 5 subdirectories in patch_dir: ['normal_001', 'tumor_001']\n",
      "[WARNING] Using ImageNet weights (not fine-tuned)\n",
      "[INFO] Loading trained classifier weights from c:\\Users\\anaca\\Documents\\sexto.curso\\tfg info\\fresh-clone\\ss25_Hierarchical_Multiscale_Image_Classification\\src\\models\\resnet18_patch_classifier.pth\n",
      "[WARNING] Using ImageNet weights (not fine-tuned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 35/35 [01:38<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Features saved to patch_features_3.npy, labels to patch_labels_3.npy, paths to patch_paths_3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract_patches(patch_size=224, level=3, stride=224, pad=True)\n",
    "# train_resnet_classifier(level=3)\n",
    "extract_features(level=3, model_path=\"resnet18_patch_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secure-sight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
